{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "def preprocess_data(data_path):\n",
    "    data = pd.read_csv(data_path)\n",
    "    # Normalize the GPS coordinates\n",
    "    scaler = MinMaxScaler()\n",
    "    data[['latitude', 'longitude']] = scaler.fit_transform(data[['latitude', 'longitude']])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_back_dataset(data, look_back_period):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - look_back_period - 1):\n",
    "        X.append(data[i:(i + look_back_period), 0])\n",
    "        y.append(data[i + look_back_period, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Construct the dataset path \n",
    "current_dir = os.path.dirname(__file__)\n",
    "dataset_path = os.path.join(current_dir, '..', 'data', 'ngsim_dataset.csv')\n",
    "#Load and preprocess data\n",
    "ngsim_data = preprocess_data('\\path\\ngsim_dataset.csv')\n",
    "\n",
    "# Create look-back dataset\n",
    "look_back_period = 5\n",
    "X, y = create_look_back_dataset(ngsim_data.values, look_back_period)\n",
    "\n",
    "# Reshape X for model input, assuming single feature\n",
    "X = X.reshape((X.shape[0], look_back_period, ngsim_data.shape[1])) \n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Architecture\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 10  # Length of the latent vector\n",
    "look_back_period = 5  # Sliding window size\n",
    "learning_rate = 0.0001  # Learning rate for the optimizer\n",
    "epochs = 75  # Number of training epochs\n",
    "batch_size = 64  # Batch size for training\n",
    "\n",
    "# Weights for the loss functions in the generator-discriminator model\n",
    "gamma_1 = 0.8\n",
    "gamma_2 = 0.2\n",
    "gamma_3 = 0.1\n",
    "\n",
    "def build_encoder(input_shape):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Conv1D(64, kernel_size=3, activation='relu', padding='same'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Conv1D(128, kernel_size=3, activation='relu', padding='same'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_decoder(latent_dim):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(latent_dim,)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Reshape((latent_dim // 16, 16)),\n",
    "        layers.Conv1DTranspose(128, kernel_size=3, activation='relu', padding='same'),\n",
    "        layers.UpSampling1D(size=2),\n",
    "        layers.Conv1DTranspose(64, kernel_size=3, activation='relu', padding='same'),\n",
    "        layers.UpSampling1D(size=2),\n",
    "        layers.Conv1DTranspose(1, kernel_size=3, activation='sigmoid', padding='same'),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_discriminator(input_shape):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Conv1D(64, kernel_size=3, activation='relu', padding='same'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Conv1D(128, kernel_size=3, activation='relu', padding='same'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid'),\n",
    "    ])\n",
    "    model.compile(optimizer=RMSprop(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    gan_input = layers.Input(shape=(generator.input_shape[1:]))\n",
    "    generated_data = generator(gan_input)\n",
    "    gan_output = discriminator(generated_data)\n",
    "    gan = models.Model(gan_input, gan_output)\n",
    "    gan.compile(optimizer=RMSprop(learning_rate=learning_rate), loss='binary_crossentropy')\n",
    "    return gan\n",
    "\n",
    "input_shape = (look_back_period, 1)\n",
    "encoder = build_encoder(input_shape)\n",
    "decoder = build_decoder(latent_dim)\n",
    "\n",
    "# Generator and Discriminator\n",
    "generator = models.Sequential([encoder, decoder])\n",
    "discriminator = build_discriminator((look_back_period, 1))\n",
    "\n",
    "gan = build_gan(generator, discriminator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss Fucntion\n",
    "\n",
    "def gan_loss(y_true, y_pred):\n",
    "    loss_1 = gamma_1 * tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    loss_2 = gamma_2 * tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "    loss_3 = gamma_3 * tf.keras.losses.mean_absolute_error(y_true, y_pred)\n",
    "    return loss_1 + loss_2 + loss_3\n",
    "\n",
    "# Applying loss for the generator\n",
    "generator.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=gan_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "\n",
    "def train_gan(gan, generator, discriminator, data, epochs=epochs, batch_size=batch_size):\n",
    "    for epoch in range(epochs):\n",
    "        # Sample random batch of real data\n",
    "        real_data = data[np.random.randint(0, data.shape[0], size=batch_size)]\n",
    "\n",
    "        # Generate fake data\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        fake_data = generator.predict(noise)\n",
    "\n",
    "        # Train the discriminator\n",
    "        d_loss_real = discriminator.train_on_batch(real_data, np.ones((batch_size, 1)))\n",
    "        d_loss_fake = discriminator.train_on_batch(fake_data, np.zeros((batch_size, 1)))\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # Train the generator\n",
    "        g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "\n",
    "       \n",
    "        if epoch % 1000 == 0:\n",
    "            print(f\"{epoch}/{epochs} [D loss: {d_loss}] [G loss: {g_loss}]\")\n",
    "\n",
    "train_gan(gan, generator, discriminator, X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "def evaluate_model(generator, real_data, fake_data):\n",
    "    # Generating the  predictions\n",
    "    real_labels = np.ones((real_data.shape[0], 1))\n",
    "    fake_labels = np.zeros((fake_data.shape[0], 1))\n",
    "\n",
    "    predictions = np.vstack((real_labels, fake_labels))\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(predictions, generator.predict(fake_data))\n",
    "    auc_pr = auc(recall, precision)\n",
    "    return auc_pr\n",
    "\n",
    "fake_data = generator.predict(np.random.normal(0, 1, (1000, latent_dim)))\n",
    "auc_pr = evaluate_model(generator, X, fake_data)\n",
    "print(f\"AUC-PR: {auc_pr}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
